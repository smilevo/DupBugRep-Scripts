# -*- coding: utf-8 -*-
"""Rania_Word2Vec_Part2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EVBl8fB2Oel_09ekpfsDUhxQCVKcVd5J

# ***Text vectorization***
"""

import pandas as pd
import matplotlib.pyplot as plt
import re
import time
import warnings
import numpy as np
from nltk.corpus import stopwords
from sklearn.preprocessing import normalize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
warnings.filterwarnings("ignore")
import sys
from scipy.sparse import hstack
import os , pickle
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
from tqdm import tqdm

import spacy

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

try:
  from google.colab import drive
  IN_COLAB=True
except:
  IN_COLAB=False

if IN_COLAB:
  print("We're running Colab")

if IN_COLAB:
  # Mount the Google Drive at mount
  mount='/content/gdrive'
  print("Colab: mounting Google drive on ", mount)

  drive.mount(mount)

os.chdir(mount +'/MyDrive/RBC_V4')

#df = pd.read_csv("/content/gdrive/MyDrive/W2VEC_PullRequest2/data_with_preprocess_2.csv")
df = pd.read_csv(mount +'/MyDrive/RBC_V4/W2V_VR4.csv', delimiter=';', encoding='cp437')
df['PR1'] = df['PR1'].apply(lambda x: str(x))
df['PR2'] = df['PR2'].apply(lambda x: str(x))
print(df.shape)
df.head(2)

"""# ***For the ease of computation we will sample only 100k points***"""

#sampling 100k
#df = df.sample(n=100000,random_state=40)

#changing columns to numeric type
num_cols = df.drop(columns=['id', 'qid1', 'qid2', 'PR1', 'PR2']).columns
for i in num_cols:
    df[i] = df[i].apply(pd.to_numeric)

y = df['is_duplicate']
X = df[df.drop(columns=['id', 'qid1', 'qid2','is_duplicate']).columns.tolist()]
print(X.shape)
print(y.shape)

X_train = X[:2662]
y_train = y[:2662]
X_test = X[2662:]
y_test = y[2662:]

"""# ***Train test split***"""

#X_train,X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2,random_state=100)
print("Number of data points in train data :",X_train.shape)
print("Number of data points in test data :",X_test.shape)

y_test

y_train

X_test

pd.set_option('display.max_rows',X_test.shape[0]+1)
X_test

X_train

"""# ***Handling text data***

We have already cleaned the text data. Now we have to vectorize it. We mainly used 2 approached.

1. TFIDF
2. TFIDF weighted glove vectorization

# ***TFIDF***
"""

tfidf_vectorizer1 = TfidfVectorizer(lowercase=False,max_features= 150)
trainqs1_tfidf = tfidf_vectorizer1.fit_transform(X_train['PR1'])
testqs1_tfidf  = tfidf_vectorizer1.transform(X_test['PR1'])
print(trainqs1_tfidf.shape)
print(testqs1_tfidf.shape)

tfidf_vectorizer2 = TfidfVectorizer(lowercase=False,max_features= 150)
train_qs2_tfidf = tfidf_vectorizer2.fit_transform(X_train['PR2'])
test_qs2_tfidf  = tfidf_vectorizer2.transform(X_test['PR2'])
print(train_qs2_tfidf.shape)
print(test_qs2_tfidf.shape)

#Now we will hstack both the vectors
tfidf_train_vec = hstack((trainqs1_tfidf,train_qs2_tfidf))
tfidf_test_vec = hstack((testqs1_tfidf,test_qs2_tfidf)) 
print("train data shape",tfidf_train_vec.shape)
print("Test data shape ",tfidf_test_vec.shape)

# selecting other features
train_df = X_train.drop(columns=['PR1', 'PR2'])
test_df = X_test.drop(columns=['PR1', 'PR2'])

#we need to convert our data with features into sparse matrix so that we can combine our feature matrix and and tfidf vectors 
import scipy
train_sparse = scipy.sparse.csr_matrix(train_df)
test_sparse = scipy.sparse.csr_matrix(test_df)

# Now combining our tfidf and features into one 
tfidf_X_tr = hstack((train_sparse,tfidf_train_vec))
tfidf_X_test = hstack((test_sparse,tfidf_test_vec))
print("train data shape",tfidf_X_tr.shape)
print("Test data shape ",tfidf_X_test.shape)

#saving tfidf vectors
pickle.dump(tfidf_train_vec, open("/content/gdrive/MyDrive/RBC_V4/tfidf_X_tr","wb"))
pickle.dump(tfidf_test_vec, open("/content/gdrive/MyDrive/RBC_V4/tfidf_X_test","wb"))

"""# ***TFIDF Weighted Glove Vectors***"""

# use spacy embedding
# run this from a normal command line
# !python -m spacy download en_core_web_md

# merge texts
PRs = list(X_train['PR1']) + list(X_train['PR2'])
tfidf = TfidfVectorizer(lowercase=False)
tfidf.fit_transform(PRs)

# dict key:word and value:tf-idf score
word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))

# Load the spacy model that you have installed
import en_core_web_sm
nlp = en_core_web_sm.load()

# each vector will be of length 94..
doc = nlp("This is some text that I am processing with Spacy")
#example
doc[3].vector.shape

nlp = spacy.load('en_core_web_sm')

vecs1 = []
# https://github.com/noamraph/tqdm
# tqdm is used to print the progress bar
for qu1 in tqdm(list(X_train['PR1'])):
    doc1 = nlp(qu1) 
    # 384 is the number of dimensions of vectors 
    mean_vec1 = np.zeros([len(doc1), 96])
    for i,word1 in enumerate(doc1):
        # word2vec
        vec1 = word1.vector
        # fetch df score
        try:
            idf = word2tfidf[str(word1)]
        except:
            idf = 0
        # compute final vec
        mean_vec1[i] += vec1 * idf
    mean_vec1 = mean_vec1.mean(axis=0)
    vecs1.append(mean_vec1)

X_train_glove_q1 = vecs1

nlp = spacy.load('en_core_web_sm')

vecs1 = []
# https://github.com/noamraph/tqdm
# tqdm is used to print the progress bar
for qu1 in tqdm(list(X_train['PR2'])):
    doc1 = nlp(qu1) 
    # 384 is the number of dimensions of vectors 
    mean_vec1 = np.zeros([len(doc1), 96])
    for i,word1 in enumerate(doc1):
        # word2vec
        vec1 = word1.vector
        # fetch df score
        try:
            idf = word2tfidf[str(word1)]
        except:
            idf = 0
        # compute final vec
        mean_vec1[i] += vec1 * idf
    mean_vec1 = mean_vec1.mean(axis=0)
    vecs1.append(mean_vec1)

X_train_glove_q2 = vecs1

nlp = spacy.load('en_core_web_sm')

vecs1 = []
# https://github.com/noamraph/tqdm
# tqdm is used to print the progress bar
for qu1 in tqdm(list(X_test['PR1'])):
    doc1 = nlp(qu1) 
    # 384 is the number of dimensions of vectors 
    mean_vec1 = np.zeros([len(doc1), 96])
    for i,word1 in enumerate(doc1):
        # word2vec
        vec1 = word1.vector
        # fetch df score
        try:
            idf = word2tfidf[str(word1)]
        except:
            idf = 0
        # compute final vec
        mean_vec1[i] += vec1 * idf
    mean_vec1 = mean_vec1.mean(axis=0)
    vecs1.append(mean_vec1)

X_test_glove_q1 = vecs1

nlp = spacy.load('en_core_web_sm')

vecs1 = []
# https://github.com/noamraph/tqdm
# tqdm is used to print the progress bar
for qu1 in tqdm(list(X_test['PR2'])):
    doc1 = nlp(qu1) 
    # 384 is the number of dimensions of vectors 
    mean_vec1 = np.zeros([len(doc1), 96])
    for i,word1 in enumerate(doc1):
        # word2vec
        vec1 = word1.vector
        # fetch df score
        try:
            idf = word2tfidf[str(word1)]
        except:
            idf = 0
        # compute final vec
        mean_vec1[i] += vec1 * idf
    mean_vec1 = mean_vec1.mean(axis=0)
    vecs1.append(mean_vec1)

X_test_glove_q2 = vecs1

X_train['q1_glove'] = X_train_glove_q1
X_train['q2_glove'] = X_train_glove_q2
X_test['q1_glove'] = X_test_glove_q1
X_test['q2_glove'] = X_test_glove_q2

train_glove = np.concatenate([np.array(X_train_glove_q1),np.array(X_train_glove_q2)],axis=1)
test_glove = np.concatenate([np.array(X_test_glove_q1),np.array(X_test_glove_q2)],axis=1)
train_glove.shape

glove_train_df = pd.DataFrame(train_glove,columns=[f'g_{i}' for i in range(train_glove.shape[1])])
glove_test_df = pd.DataFrame(test_glove,columns=[f'g_{i}' for i in range(test_glove.shape[1])])
glove_train_df.head()

X_train = X_train.drop(columns=['PR1','PR2']).reset_index(drop=True)
X_test = X_test.drop(columns=['PR1','PR2']).reset_index(drop=True)
print(X_train.shape)
print(X_test.shape)

# concatenating
X_train_d = pd.concat([X_train,glove_train_df],axis=1)
X_test_d = pd.concat([X_test,glove_test_df],axis=1)
print(X_train_d.shape)
print(X_test_d.shape)

X_train_d.to_csv('/content/gdrive/MyDrive/RBC_V4/train_data.csv',index=False)
X_test_d.to_csv('/content/gdrive/MyDrive/RBC_V4/test_data.csv',index=False)

y_train.to_csv('/content/gdrive/MyDrive/RBC_V4/train_y.csv',index=False)
y_test.to_csv('/content/gdrive/MyDrive/RBC_V4/test_y.csv',index=False)